{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "This notebook will walk through applying an ML model to data it was not trained on, to show you the sorts of things that can go wrong when using models on out of distribution data.\n",
    "\n",
    "The model we are using here was trained on Lung Cancer patients who were imaged in their radiotherapy treatment position for treatment planning. This is a very controlled environment, with well calibrated CT machines, consistent voxel dimensions, and fairly consistent anatomy.\n",
    "\n",
    "In the images we will test with here, we have a lot of differences:\n",
    "- Some patients with active COVID infections\n",
    "- Some patients imaged with/without contrast\n",
    "- Variable imaging dose (diagnostic CT vs PET-CT)\n",
    "\n",
    "\n",
    "I didn't test the model on any of these, so I have no idea if it will perform well or not. I would expect it to fall over a fair bit, but equally we might be surprised how resilient it is.\n",
    "\n",
    "To run this notebook, start at the top and click the play button beside every chunk of code. This will install the necessary libraries and import them for use, then define a bunch of helper functions. \n",
    "\n",
    "Once the cell at the bottom is reached, you should be able to change the file path and see how the model behaves on the different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install albumentations pydicom pytorch-lightning\n",
    "%pip install git+https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as ipyw\n",
    "import torch\n",
    "import os\n",
    "import albumentations as A\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import pydicom\n",
    "\n",
    "structure_names = ['SpinalCord', 'Lung_R', 'Lung_L', 'Heart', 'Esophagus']\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  datapath = \"/content/\"\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  datapath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(slices_fpath):\n",
    "    \"\"\"\n",
    "    This functions load the DICOM corresponding to an image, but doesn't actually load the pixels. \n",
    "    Instead, we just keep copies of the whole DICOM object for each slice, which then includes other stuff.\n",
    "    Crucially, we sort the returned list on the patient's position so that the slices and other returned \n",
    "    things are in the correct order\n",
    "    \"\"\"\n",
    "    slices = []\n",
    "    for slice_fname in  os.listdir(slices_fpath):\n",
    "        try:\n",
    "            slice_f = pydicom.dcmread(os.path.join(slices_fpath, slice_fname))\n",
    "            slice_f.pixel_array ## in case there's an RTSTRUCT\n",
    "            assert slice_f.Modality != \"RTDOSE\"\n",
    "            slices.append(slice_f)\n",
    "        except:\n",
    "            continue\n",
    "    slices = sorted(slices, key=lambda s: s.ImagePositionPatient[-1])\n",
    "    uids = [s.SOPInstanceUID for s in slices]\n",
    "    pixels = np.array([(float(s.PixelSpacing[0]), float(s.PixelSpacing[1]))  for s in slices])\n",
    "    origins = np.array([s.ImagePositionPatient for s in slices])\n",
    "\n",
    "\n",
    "    img_array = np.zeros((len(slices), *slices[0].pixel_array.shape) , dtype=np.int16)\n",
    "    \n",
    "    ## Very important to apply the rescale & intercept - pydicom doesn't do it by default\n",
    "    for idx, img_slice in enumerate(slices):\n",
    "        img_array[idx, ...] = pydicom.pixel_data_handlers.apply_rescale(img_slice.pixel_array, img_slice)\n",
    "\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_level(data, window=350, level=50):\n",
    "    \"\"\"\n",
    "    Apply a window and level transformation to CT slices. \n",
    "\n",
    "    The default values are taken taken from https://radiopaedia.org/articles/windowing-ct?lang=gb and are recommended for visualising the mediastinum\n",
    "    \n",
    "    The returned array will be NxHxWx3, as we expand the array into 3 channels. Values will be in the range 0-255 and type will be uint8 to mimic a 'normal' image\n",
    "    \"\"\"\n",
    "    ## calculate high & low edges of level & window\n",
    "    low_edge  = level - (window//2)\n",
    "    high_edge = level + (window//2)\n",
    "    ## use np.clip to clip into that level/window, then adjust to range 0 - 255 and convert to uint8\n",
    "    windowed_data = (((np.clip(data, low_edge, high_edge) - low_edge)/window) * 255).astype(np.uint8)\n",
    "    \n",
    "    return windowed_data\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=(np.mean([0.485, 0.456, 0.406])), std=(np.mean([0.229, 0.224, 0.225]))) ## Note mean of means, mean of stds\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSliceViewer3D:\n",
    "    \"\"\" \n",
    "    ImageSliceViewer3D is for viewing volumetric image slices in jupyter or\n",
    "    ipython notebooks. \n",
    "    \n",
    "    User can interactively change the slice plane selection for the image and \n",
    "    the slice plane being viewed. \n",
    "\n",
    "    Argumentss:\n",
    "    Volume = 3D input image\n",
    "    figsize = default(8,8), to set the size of the figure\n",
    "    cmap = default('gray'), string for the matplotlib colormap. You can find \n",
    "    more matplotlib colormaps on the following link:\n",
    "    https://matplotlib.org/users/colormaps.html\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, volume, mask, figsize=(10,10), cmap='gray'):\n",
    "        self.volume = volume\n",
    "        self.mask = mask\n",
    "        self.figsize = figsize\n",
    "        self.cmap = cmap\n",
    "        self.v = [np.min(volume), np.max(volume)]\n",
    "        \n",
    "        # Call to select slice plane\n",
    "        ipyw.interact(self.views)\n",
    "    \n",
    "    def views(self):\n",
    "        self.vol1 = np.transpose(self.volume, [1,2,0])\n",
    "        self.mask1 = np.transpose(self.mask, [1,2,0])\n",
    "        maxZ1 = self.vol1.shape[2] - 1\n",
    "        ipyw.interact(self.plot_slice, \n",
    "            z1=ipyw.IntSlider(min=0, max=maxZ1, step=1, continuous_update=False, \n",
    "            description='Axial:'),)\n",
    "    \n",
    "    def plot_slice(self, z1):\n",
    "        # Plot slice for the given plane and slice\n",
    "        f,ax = plt.subplots(1,1, figsize=self.figsize)\n",
    "        #print(self.figsize)\n",
    "        #self.fig = plt.figure(figsize=self.figsize)\n",
    "        #f(figsize = self.figsize)\n",
    "        ax[0].imshow(self.vol1[:,:,z1], cmap=plt.get_cmap(self.cmap), \n",
    "            vmin=self.v[0], vmax=self.v[1])\n",
    "        ax[0].imshow(self.mask1[:,:,z1], alpha=0.75, cmap='Pastel2', vmin=1, vmax=5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the class that will wrap te pytorch model up for ptl\n",
    "class LightningFPN(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    ## Create the pytorch model \n",
    "    self.model = smp.FPN(\"resnet18\", in_channels=1, classes=len(structure_names)+1, encoder_weights='imagenet')\n",
    "    \n",
    "    ## Construct a loss function, this is DSC, configured for multiple classes, and ignoring the background\n",
    "    self.loss_fcn = smp.losses.DiceLoss(\"multiclass\", from_logits=True)\n",
    "\n",
    "    ## Specify which optimiser to use here\n",
    "    self.optimizer = torch.optim.Adam\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = self.optimizer(self.parameters(), lr=1e-4)## May need to handle other kwargs here!\n",
    "    return {\"optimizer\": optimizer, \"reduce_on_plateau\":True}\n",
    "    ## Note - we are reducing the learning rate when the validation loss plateaus for a while - this should improve the model\n",
    "\n",
    "  def predict(self, x):\n",
    "    return self.model.predict(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    img, msk = batch\n",
    "    msk_hat = self(img)\n",
    "    loss = self.loss_fcn(msk_hat, msk.long())\n",
    "    self.log(\"loss\", loss)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    img, msk = batch\n",
    "    msk_hat = self(img)\n",
    "    val_loss = self.loss_fcn(msk_hat, msk.long())\n",
    "    self.log(\"val_loss\", val_loss)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "## Now we can wrap the prebuilt model up inside a pytorch lightning module:\n",
    "\n",
    "pl_model = LightningFPN()\n",
    "\n",
    "## Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_3d(image, model, transforms):\n",
    "    batch_size = 8\n",
    "    whole_batches = image.shape[0] // batch_size\n",
    "    batch_splitpoints = [(a*batch_size, a*batch_size + batch_size) for a in  range(whole_batches)]\n",
    "\n",
    "    ## do things on GPU\n",
    "    model.cuda()\n",
    "\n",
    "    if image.shape[0] % batch_size != 0:\n",
    "        last_batch_start_idx = whole_batches * batch_size\n",
    "        last_batch_size = image.shape[0] - last_batch_start_idx\n",
    "        batch_splitpoints.append((last_batch_start_idx, image.shape[0]))\n",
    "    \n",
    "    segmentation = np.zeros_like(image)\n",
    "    for b_start, b_stop in batch_splitpoints:\n",
    "        transformed_image = torch.tensor(transforms(image=image[b_start:b_stop])['image']).cuda()\n",
    "        logits = model.predict(transformed_image[:, np.newaxis,...]).cpu()\n",
    "        probs = torch.nn.functional.softmax(logits)\n",
    "        segmentation[b_start:b_stop,...] = np.argmax(probs, axis=1)\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_dicom(dicom_path, model):\n",
    "    \"\"\"\n",
    "    Does all the loading and preprocessing for you!\n",
    "    \"\"\"\n",
    "    image = load_image(dicom_path)\n",
    "    wl_image = window_level(image)\n",
    "    segmentation = segment_3d(wl_image, model, val_transforms)\n",
    "\n",
    "    return image, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we download the pretrained model weights for this model\n",
    "!wget https://www.dropbox.com/s/sbgmtd7t344iklx/pretrained_checkpoint.ckpt?dl=0 -O pretrained_checkpoint.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First load a pretrained model from the checkpoint, this should allow us to segment things\n",
    "\n",
    "model = pl_model.load_from_checkpoint(os.path.join(datapath, \"pretrained_checkpoint.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we download and extract the test images I prepared\n",
    "!wget https://www.dropbox.com/s/b42o3o8bpzt55pv/TestImages.tar.gz?dl=0 -O TestImages.tar.gz\n",
    "!tar -xf TestImages.tar.gz\n",
    "!rm TestImages.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the file paths in the browser on the left, this should work for the first example\n",
    "image, segmentation = segment_dicom(\"/content/TestImages/Image001\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now visualise it\n",
    "vw = ImageSliceViewer3D(image, segmentation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try doing this with the other 9 images. Some of them should give some pretty funky results. I can show you how I downloaded these images if you would like to find more to try this with!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
